{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model, feature_extraction, naive_bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import xgboost\n",
    "from xgboost import DMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Загрузите\tдатасет\tпо\tссылке:\t\n",
    "    http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip\n",
    "    \n",
    "2.Считайте датасет в Python (можете сразу грузить все в память, выборка небольшая), выясните, что используется в качестве разделителей и как проставляются метки классов.\n",
    "\n",
    "3.Подготовьте для дальнейшей работы два списка: список текстов в порядке их следования в датасете и список соответствующих им меток классов. В качестве метки класса используйте 1 для спама и 0 для \"не спама\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection\", sep = \"\\t\", header=None)\n",
    "data.columns = [\"target\", \"text\"]\n",
    "data[\"target\"] = data.target.map({\"ham\":0, \"spam\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  Go until jurong point, crazy.. Available only ...\n",
       "1       0                      Ok lar... Joking wif u oni...\n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0  U dun say so early hor... U c already then say...\n",
       "4       0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0</td>\n",
       "      <td>May i call You later Pls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0</td>\n",
       "      <td>Hasn't that been the pattern recently crap wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0</td>\n",
       "      <td>I have a sore throat. It's scratches when I talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>0</td>\n",
       "      <td>Yes da. Any plm at ur office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>0</td>\n",
       "      <td>Are you not around or just still asleep? :V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0</td>\n",
       "      <td>Lol you forgot it eh ? Yes, I'll bring it in babe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "1024       0                           May i call You later Pls\n",
       "1025       0  Hasn't that been the pattern recently crap wee...\n",
       "1026       0   I have a sore throat. It's scratches when I talk\n",
       "1027       0                       Yes da. Any plm at ur office\n",
       "1028       0        Are you not around or just still asleep? :V\n",
       "1029       0  Lol you forgot it eh ? Yes, I'll bring it in babe"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1024:1030]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Используя sklearn.feature_extraction.text.CountVectorizer со стандартными настройками, получите из списка текстов матрицу признаков X.\n",
    "\n",
    "5.Оцените качество классификации текстов с помощью LogisticRegression() с параметрами по умолчанию, используя sklearn.cross_validation.cross_val_score и посчитав среднее арифметическое качества на отдельных fold'ах. Параметр cv задайте равным 10. В качестве метрики качества используйте f1-меру. Получившееся качество – ответ в этом пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.text\n",
    "y = data.target\n",
    "\n",
    "cnt_vec = feature_extraction.text.CountVectorizer()\n",
    "log_reg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.932640298361\n"
     ]
    }
   ],
   "source": [
    "pipe_estimator = Pipeline(steps=[('cnt_vec', cnt_vec), ('log_reg', log_reg)])\n",
    "score = cross_val_score(estimator=pipe_estimator, X=X, y=y, scoring=\"f1\", cv=10)\n",
    "print(\"score = {0}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.А\tтеперь\tобучите\tклассификатор\tна\tвсей\tвыборке\tи\tспрогнозируйте\tс\tего\t\n",
    "помощью\tкласс\tдля\tследующих\tсообщений:\n",
    "\n",
    " 1. \"FreeMsg:\tTxt:\tCALL\tto\tNo:\t86888\t&\tclaim\tyour\treward\tof\t3\thours talk\ttime\tto\tus from\tyour\tphone\tnow!\tSubscribe6GB\"\n",
    " 2. \"FreeMsg:\tTxt:\tclaim\tyour\treward\tof\t3\thours\ttalk\ttime\"\n",
    " 3. \"Have\tyou\tvisited\tthe\tlast\tlecture\ton\tphysics?\"\n",
    " 4. \"Have\tyou\tvisited\tthe\tlast\tlecture\ton\tphysics?\tJust\tbuy\tthis\tbook\tand\tyou\twill\thave all\n",
    "  materials!\tOnly\t99\\$\"\n",
    " 5. \"Only\t99$\"\n",
    "\n",
    "Выпишите\tчерез\tпробел\tпрогнозы\tклассификатора\t(0\t– не\tспам,\t1\t– спам)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame([\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "            \"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "            \"Have you visited the last lecture on physics?\",\n",
    "            \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "            \"Only 99$\"], columns = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "pipe_estimator.fit(data.text, y)\n",
    "\n",
    "print(pipe_estimator.predict(test.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Задайте\tв\tCountVectorizer\tпараметр\tngram_range=(2,2),\tзатем\t\n",
    "ngram_range=(3,3),\tзатем\tngram_range=(1,3).\t\n",
    "\n",
    "Во\tвсех\tтрех\tслучаях\tизмерьте\tполучившееся\tв\tкросс-валидации\tзначение\tf1-меры,\tокруглите\tдо\tвторого\tзнака\tпосле\tточки,\tи\tвыпишете\tрезультаты\tчерез пробел\tв\tтом\tже\tпорядке.\t\n",
    "\n",
    "В\tданном\tэксперименте\tмы\tпробовали\tдобавлять\tв\tпризнаки\tn-граммы\tдля\tразных\tдиапазонов\tn\t- только биграммы,\tтолько\tтриграммы,\tи,\tнаконец,\tвсе\tвместе\t- униграммы,\tбиграммы\tи\tтриграммы.\t\n",
    "\n",
    "Обратите\tвнимание,\tчто\tстатистики\tпо\tбиграммам\tи\tтриграммам\tнамного\tменьше,\tпоэтому\tклассификатор\t\n",
    "только\tна\tних\tработает\tхуже.\n",
    "\n",
    "В\tто\tже\tвремя\tэто\tне\tухудшает\tрезультат\tсколько-нибудь\tсущественно,\tесли\tдобавлять\tих\tвместе\tс\tуниграммами,\tт.к.\tза\tсчет\tрегуляризации\tлинейный\tклассификатор\tне\tсклонен\tсильно\tпереобучаться\tна\tэтих\tпризнаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram = (2, 2), score = 0.82\n",
      "ngram = (3, 3), score = 0.73\n",
      "ngram = (1, 3), score = 0.93\n"
     ]
    }
   ],
   "source": [
    "ngram_ar = [(2,2), (3,3), (1,3)]\n",
    "for ngram in ngram_ar:\n",
    "    cnt_vec = feature_extraction.text.CountVectorizer(ngram_range=ngram)\n",
    "    log_reg = linear_model.LogisticRegression()\n",
    "    pipe_estimator = Pipeline(steps=[('cnt_vec', cnt_vec), ('log_reg', log_reg)])\n",
    "    \n",
    "    score = cross_val_score(estimator=pipe_estimator, X=X, y=y, scoring=\"f1\", cv=10)\n",
    "    print(\"ngram = {0}, score = {1:.2f}\".format(ngram, score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Повторите\tаналогичный\tп.7\tэксперимент,\tиспользуя\tвместо\t\n",
    "логистической\tрегрессии\tMultinomialNB().\n",
    "\n",
    "Обратите\tвнимание,\tнасколько\tсильнее\t(по\tсравнению\tс\tлинейным\tклассификатором)\tнаивный\tБайес страдает\tот\tнехватки\tстатистики\tпо\tбиграммам\tи\tтриграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram = (2, 2), score = 0.93\n",
      "ngram = (3, 3), score = 0.87\n",
      "ngram = (1, 3), score = 0.95\n"
     ]
    }
   ],
   "source": [
    "ngram_ar = [(2,2), (3,3), (1,3)]\n",
    "for ngram in ngram_ar:\n",
    "    cnt_vec = feature_extraction.text.CountVectorizer(ngram_range=ngram)\n",
    "    naive_bayes = sklearn.naive_bayes.MultinomialNB()\n",
    "    pipe_estimator = Pipeline(steps=[('cnt_vec', cnt_vec), ('naive_bayes', naive_bayes)])\n",
    "    \n",
    "    score = cross_val_score(estimator=pipe_estimator, X=X, y=y, scoring=\"f1\", cv=10)\n",
    "    print(\"ngram = {0}, score = {1:.2f}\".format(ngram, score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Попробуйте использовать в логистической регрессии в качестве признаков Tf-idf из TfidfVectorizer на униграммах.\n",
    "\n",
    "Повысилось или понизилось качество на кросс-валидации по сравнению с CountVectorizer на униграммах? \n",
    "\n",
    "Обратите внимание, что результат перехода к tf-idf не всегда будет таким - если вы наблюдаете какое-то явление на одном датасете, не надо сразу же его обобщать на любые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf with unigramm = (1,1), score = 0.88\n"
     ]
    }
   ],
   "source": [
    "tf_idf = feature_extraction.text.TfidfVectorizer(ngram_range=(1,1))\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "pipe_estimator = Pipeline(steps=[('tf_idf', tf_idf), ('log_reg', log_reg)])\n",
    "\n",
    "score = cross_val_score(estimator=pipe_estimator, X=X, y=y, scoring=\"f1\", cv=10)\n",
    "print(\"Tf-idf with unigramm = (1,1), score = {0:.2f}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer with unigramm = (1,1), score = 0.93\n"
     ]
    }
   ],
   "source": [
    "cnt_vec = feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "pipe_estimator = Pipeline(steps=[('cnt_vec', cnt_vec), ('log_reg', log_reg)])\n",
    "    \n",
    "score = cross_val_score(estimator=pipe_estimator, X=X, y=y, scoring=\"f1\", cv=10)\n",
    "print(\"CountVectorizer with unigramm = (1,1), score = {0:.2f}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество кросс-валидации на униграммах понизилось при использовании Tf-idf по сравнению с CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.*Попробуйте получить как\tможно более\tвысокое\tкачество на кросс-валидации.\n",
    "Напишите, что пробовали\tи какое\tкачество получилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я попробовал СountVectorizer c xgboost, но качество 0.945 не превзошел,варьируя ngram_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.Какие\tнаблюдения\tи\tвыводы\tможно\tсделать\tиз\tэтого\tзадания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что использовать Tf-idf или CountVectorizer зависит от данного нам датасета. Точность больше при использовании униграмм, биграмм и триграмм вместе взятых, что очевидно, но иногда хорошего результата можно добиться использованием допустим только биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
